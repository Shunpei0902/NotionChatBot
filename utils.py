import streamlit as st
import openai
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferWindowMemory
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.prompts.chat import SystemMessagePromptTemplate

openai.api_key = st.secrets["OPENAI_API_KEY"]

@st.cache_resource
def load_chain():
  """
    The `load_chain()` function initializes and configures a conversational retrieval chain for
    answering user questions.
    :return: The `load_chain()` function returns a ConversationalRetrievalChain object.
    """

  # Load OpenAI embedding model
  embeddings = OpenAIEmbeddings()
  
  # Load OpenAI chat model
  llm = ChatOpenAI(temperature=0)
  
  # Load our local FAISS index as a retriever
  vector_store = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
  retriever = vector_store.as_retriever(search_kwargs={"k": 3})
  
  # Create memory 'chat_history' 
  memory = ConversationBufferWindowMemory(k=3,memory_key="chat_history")
  
  # Create system prompt
  template = """
  ã‚ãªãŸã¯ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å€‹äººãƒŠãƒ¬ãƒƒã‚¸ãƒ¡ãƒ¢ã«ã¤ã„ã¦ã®è³ªå•ã«ç­”ãˆã‚‹AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
  é•·ã„æ–‡æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸä»¥ä¸‹ã®éƒ¨åˆ†ã¨è³ªå•ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚ä¼šè©±å½¢å¼ã§ç­”ãˆã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
  ã‚‚ã—ç­”ãˆãŒã‚ã‹ã‚‰ãªã‘ã‚Œã°ã€ã€Œç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ã‚ã‹ã‚Šã¾ã›ã‚“â€¦ğŸ˜”ã€ã¨è¨€ã£ã¦ãã ã•ã„ã€‚
  ç­”ãˆã‚’ã§ã£ã¡ä¸Šã’ãªã„ã§ãã ã•ã„ã€‚
  ã‚‚ã—è³ªå•ãŒã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å€‹äººãƒŠãƒ¬ãƒƒã‚¸ãƒ¡ãƒ¢ã«é–¢ã™ã‚‹ã‚‚ã®ã§ãªã‘ã‚Œã°ã€ã‚ãªãŸãŒã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®å€‹äººãƒŠãƒ¬ãƒƒã‚¸ãƒ¡ãƒ¢ã«é–¢ã™ã‚‹è³ªå•ã®ã¿ã«ç­”ãˆã‚‹ã‚ˆã†è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ä¸å¯§ã«ä¼ãˆã¦ãã ã•ã„ã€‚
  
  {context}
  è³ªå•: {question}
  å½¹ã«ç«‹ã¤ç­”ãˆ:"""
  
  # Create the Conversational Chain
  chain = ConversationalRetrievalChain.from_llm(llm=llm, 
                                              retriever=retriever, 
                                              memory=memory, 
                                              get_chat_history=lambda h : h,
                                              verbose=True)
  
  # Add systemp prompt to chain
  # Can only add it at the end for ConversationalRetrievalChain
  QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template)
  chain.combine_docs_chain.llm_chain.prompt.messages[0] = SystemMessagePromptTemplate(prompt=QA_CHAIN_PROMPT)
  
  return chain